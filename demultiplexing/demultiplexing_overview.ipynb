{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354de444",
   "metadata": {},
   "source": [
    "# Demultiplexing the MultiSpecies Comparison data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe50bf",
   "metadata": {},
   "source": [
    "### Timeline:\n",
    "1. FASTQ files aligned to mixed reference genome using ParseNIP. <span style=\"color:#87CEEB; font-family:'Consolas', 'Menlo', 'DejaVu Sans Mono', monospace; font-weight:500\">/cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2</span>\n",
    "2. Modifying BAM files: CB tag in BAM files for each sublibrary (10 sublibraries) was surogated for a new 16-bp sequence analogous to 10X barcodes to bridge the incompatibility between cellbouncer and Parse Biosciences. <span style=\"color:#87CEEB; font-family:'Consolas', 'Menlo', 'DejaVu Sans Mono', monospace; font-weight:500\">/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/modified_bam</span>\n",
    "3. Parse–sudo10X translation dictionaries derived from the translated files. <span style=\"color:#87CEEB; font-family:'Consolas', 'Menlo', 'DejaVu Sans Mono', monospace; font-weight:500\">/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/translated_ParseWT*_barcodes.csv</span>\n",
    "4. Translated BAM files processed with utils/composite_bam2counts -b [bamfile] -o [output_directory]. <span style=\"color:#87CEEB; font-family:'Consolas', 'Menlo', 'DejaVu Sans Mono', monospace; font-weight:500\">/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/composite_bam2counts</span>\n",
    "5. Outputs processed with demux_species –o [output_directory] to get summary of species assigned for each cell barcodes (sudo10X) , the confidence and singlet/doublet score.\n",
    "6. Created a metadata with parse barcodes, sudo10x barcode, singlet/doublet score, confidence, species assigned. <span style=\"color:#87CEEB; font-family:'Consolas', 'Menlo', 'DejaVu Sans Mono', monospace; font-weight:500\">/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/demultiplexed/metadata_barcode_species_ParseWT*.csv</span>\n",
    "7. Created list of barcodes of each species for each sublibrary. <span style=\"color:#87CEEB; font-family:'Consolas', 'Menlo', 'DejaVu Sans Mono', monospace; font-weight:500\">/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/demultiplexed/ParseWT__barcodes.csv</span>\n",
    "8. Filtering FASTQ files into separate species. \\\n",
    "8.1 Making lists of headers corresponding to each species per sublibarary. <span style=\"color:#87CEEB; font-family:'Consolas', 'Menlo', 'DejaVu Sans Mono', monospace; font-weight:500\">/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/demultiplexed/Fastq_filters</span> \\\n",
    "8.2 Concatanting FASTQ of each sublibrary across sequencing lanes. <span style=\"color:#87CEEB; font-family:'Consolas', 'Menlo', 'DejaVu Sans Mono', monospace; font-weight:500\">/cephfs2/hannas/MultiSpeciesComp/final/expdata/Fastq_merged</span> \\\n",
    "8.3 Filtering FASTQ files into separate species. <span style=\"color:#87CEEB; font-family:'Consolas', 'Menlo', 'DejaVu Sans Mono', monospace; font-weight:500\">/cephfs2/hannas/MultiSpeciesComp/final/expdata/Fastq_filtered</span>\n",
    "9. Realignment of species FASTQ to species-specific reference genomes using ParseNIP. \n",
    "10. CrossFilt removal of annotation biases from resulting BAM files: crossfilt-lift.\n",
    "11. Turning BAM files into counts matricies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe56e31",
   "metadata": {},
   "source": [
    "#### 1. FASTQ file aligned to mixed regerence genome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f805bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mkaing mixed reference genome \n",
    "nextflow run -config /public/singularity/containers/nextflow/lmb-nextflow/genomes.config,/public/singularity/containers/nextflow/ParseNIP/nextflow.config \\\n",
    "    -profile lmb_cluster /public/singularity/containers/nextflow/ParseNIP/main.nf \\\n",
    "    --fasta Ensembl/homo_sapiens/GRCh38/Release_102/FASTA/homo_sapiens__GRCh38__release102.dna.fa,Ensembl/pan_troglodytes/Pan_tro_3.0/Release_105/FASTA/pan_troglodytes__Pan_tro_3.0__release105.dna.fa,Ensembl/gorilla_gorilla/gorGor4/Release_110/FASTA/gorilla_gorilla__gorGor4__release110.dna.fa,test_genome/Macaca_fascicularis_test.fa,Ensembl/mus_musculus/GRCm39/Release_108/FASTA/mus_musculus__GRCm39__release108.dna.fa \\\n",
    "    --gtf Ensembl/homo_sapiens/GRCh38/Release_102/GTF/Homo_sapiens.GRCh38.102.gtf,Ensembl/pan_troglodytes/Pan_tro_3.0/Release_105/GTF/Pan_troglodytes.Pan_tro_3.0.105.gtf,Ensembl/gorilla_gorilla/gorGor4/Release_110/GTF/Gorilla_gorilla.gorGor4.110.gtf,Ensembl/macaca_fascicularis/Macaca_fascicularis_6.0/Release_112/GTF/Macaca_fascicularis.Macaca_fascicularis_6.0.112.gtf,Ensembl/mus_musculus/GRCm39/Release_108/GTF/Mus_musculus.GRCm39.108.gtf \\\n",
    "    --genome_name GRCh38,Pan_tro_3.0,gorGor4,Macaca_fascicularis_6.0,GRCm39 -bg\n",
    "\n",
    "#alignment with ParseNIP\n",
    "nextflow run -config /public/singularity/containers/nextflow/lmb-nextflow/genomes.config,/public/singularity/containers/nextflow/ParseNIP/nextflow.config \\\n",
    "-profile lmb_cluster /public/singularity/containers/nextflow/ParseNIP/main.nf \\\n",
    "--genome_dir /cephfs2/hannas/MultiSpeciesComp/final/genome/mixed_ref_genome/results/GENOME_INDEX \\\n",
    "--samp_list /cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/SampleLoadingTable_MultiSpieciesComp.txt \\\n",
    "--fastq /cephfs2/hannas/MultiSpeciesComp/final/expdata/Fastq --chemistry v1 --concatenate -bg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc3496",
   "metadata": {},
   "source": [
    "#### 2. Modifying BAM files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import random\n",
    "\n",
    "# Your exact functions (unchanged)\n",
    "def extract_unique_barcodes_from_bam(bam_file_path, output_file=\"unique_barcodes_subset1000.txt\"):\n",
    "    unique_barcodes = set()\n",
    "    with pysam.AlignmentFile(bam_file_path, \"rb\") as bam_file:\n",
    "        for read in bam_file:\n",
    "            if read.has_tag('CB'):\n",
    "                cb_tag = read.get_tag('CB')\n",
    "                unique_barcodes.add(cb_tag)\n",
    "        unique_barcode_list = sorted(list(unique_barcodes))\n",
    "        with open(output_file, 'w') as f:\n",
    "            for barcode in unique_barcode_list:\n",
    "                f.write(f\"{barcode}\\n\")\n",
    "        return unique_barcode_list\n",
    "\n",
    "def generate_unique_16bp_sequences(num_sequences):\n",
    "    unique_sequences = set()\n",
    "    while len(unique_sequences) < num_sequences:\n",
    "        sequence = ''.join(random.choice('ACGT') for _ in range(16))\n",
    "        unique_sequences.add(sequence)\n",
    "    return list(unique_sequences)\n",
    "\n",
    "def new_barcodes_bam(input_bam, output_bam, translation_dict):\n",
    "    with pysam.AlignmentFile(input_bam, \"rb\") as input_file:\n",
    "        with pysam.AlignmentFile(output_bam, \"wb\", header=input_file.header) as output_file:\n",
    "            for read in input_file:\n",
    "                if read.has_tag('CB'):\n",
    "                    old_barcode = read.get_tag('CB')\n",
    "                    if old_barcode in translation_dict:\n",
    "                        new_barcode = translation_dict[old_barcode]\n",
    "                        read.set_tag('CB', new_barcode, 'Z')\n",
    "                output_file.write(read)\n",
    "\n",
    "# Pipeline\n",
    "bam_path = \"/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/subset_1000.bam\"\n",
    "\n",
    "unique_barcode_list = extract_unique_barcodes_from_bam(bam_path)\n",
    "new_sequences = generate_unique_16bp_sequences(len(unique_barcode_list))\n",
    "translation_dict = dict(zip(unique_barcode_list, new_sequences))\n",
    "new_barcodes_bam(bam_path, \"translated_output.bam\", translation_dict)\n",
    "\n",
    "# List of BAM files\n",
    "# List of BAM files\n",
    "bam_files = [\n",
    "    \"/cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/results/splitpipe/SLX-25498.ParseWT21.22K75GLT4/process/barcode_headAligned_anno.bam\",\n",
    "    \"/cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/results/splitpipe/SLX-25498.ParseWT22.22K75GLT4/process/barcode_headAligned_anno.bam\",\n",
    "    \"/cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/results/splitpipe/SLX-25498.ParseWT33.22K75GLT4/process/barcode_headAligned_anno.bam\",\n",
    "    \"/cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/results/splitpipe/SLX-25498.ParseWT34.22K75GLT4/process/barcode_headAligned_anno.bam\",\n",
    "    \"/cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/results/splitpipe/SLX-25498.ParseWT35.22K75GLT4/process/barcode_headAligned_anno.bam\",\n",
    "    \"/cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/results/splitpipe/SLX-25498.ParseWT36.22K75GLT4/process/barcode_headAligned_anno.bam\",\n",
    "    \"/cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/results/splitpipe/SLX-25498.ParseWT37.22K75GLT4/process/barcode_headAligned_anno.bam\",\n",
    "    \"/cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/results/splitpipe/SLX-25498.ParseWT38.22K75GLT4/process/barcode_headAligned_anno.bam\",\n",
    "    \"/cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/results/splitpipe/SLX-25498.ParseWT39.22K75GLT4/process/barcode_headAligned_anno.bam\",\n",
    "    \"/cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/results/splitpipe/SLX-25498.ParseWT40.22K75GLT4/process/barcode_headAligned_anno.bam\"\n",
    "]\n",
    "\n",
    "# Core names for output files\n",
    "output_names = [\n",
    "    \"ParseWT21\",\n",
    "    \"ParseWT22\",\n",
    "    \"ParseWT33\",\n",
    "    \"ParseWT34\",\n",
    "    \"ParseWT35\",\n",
    "    \"ParseWT36\",\n",
    "    \"ParseWT37\",\n",
    "    \"ParseWT38\",\n",
    "    \"ParseWT39\",\n",
    "    \"ParseWT40\"\n",
    "]\n",
    "# Process each file\n",
    "for i, bam_path in enumerate(bam_files):\n",
    "    core_name = output_names[i]\n",
    "    \n",
    "    unique_barcode_list = extract_unique_barcodes_from_bam(bam_path, f\"unique_barcodes_{core_name}.txt\")\n",
    "    new_sequences = generate_unique_16bp_sequences(len(unique_barcode_list))\n",
    "    translation_dict = dict(zip(unique_barcode_list, new_sequences))\n",
    "    new_barcodes_bam(bam_path, f\"translated_{core_name}.bam\", translation_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582838dc",
   "metadata": {},
   "source": [
    "### 3. Parse–sudo10X translation dictionaries derived from the translated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pysam\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Find all BAM files\n",
    "bam_files = glob.glob(\"/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/modified_bam/*.bam\")\n",
    "\n",
    "print(f\"Found {len(bam_files)} BAM files\")\n",
    "\n",
    "# Process each BAM file\n",
    "for bam_file in bam_files:\n",
    "    print(f\"Processing {os.path.basename(bam_file)}...\")\n",
    "    \n",
    "    # Create dictionary for this file\n",
    "    cb_dict = {}\n",
    "    with pysam.AlignmentFile(bam_file, \"rb\") as bam:\n",
    "        for read in bam:\n",
    "            if read.has_tag(\"CB\"):\n",
    "                cb_tag = read.get_tag(\"CB\")\n",
    "                if cb_tag not in cb_dict:\n",
    "                    cb_dict[cb_tag] = read.query_name[:8]\n",
    "    \n",
    "    # Save as CSV\n",
    "    base_name = os.path.splitext(os.path.basename(bam_file))[0]\n",
    "    output_csv = f\"{base_name}_barcodes.csv\"\n",
    "    \n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['cell_barcode', 'read_name'])\n",
    "        for cb, read_name in cb_dict.items():\n",
    "            writer.writerow([cb, read_name])\n",
    "    \n",
    "    print(f\"  Saved {len(cb_dict)} barcodes to {output_csv}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b22f370",
   "metadata": {},
   "source": [
    "### 4. Translated BAM files processed with utils/composite_bam2counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils/composite_bam2counts -b/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/translated_ParseWT21.bam -o /cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/composite_bam2counts\n",
    "utils/composite_bam2counts -b/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/translated_ParseWT22.bam -o /cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/composite_bam2counts/ParseWT22\n",
    "utils/composite_bam2counts -b/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/translated_ParseWT33.bam -o /cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/composite_bam2counts/ParseWT33\n",
    "utils/composite_bam2counts -b/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/translated_ParseWT34.bam -o /cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/composite_bam2counts/ParseWT34\n",
    "utils/composite_bam2counts -b/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/translated_ParseWT35.bam -o /cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/composite_bam2counts/ParseWT35\n",
    "utils/composite_bam2counts -b/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/translated_ParseWT36.bam -o /cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/composite_bam2counts/ParseWT36\n",
    "utils/composite_bam2counts -b/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/translated_ParseWT37.bam -o /cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/composite_bam2counts/ParseWT37\n",
    "utils/composite_bam2counts -b/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/translated_ParseWT38.bam -o /cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/composite_bam2counts/ParseWT38\n",
    "utils/composite_bam2counts -b/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/translated_ParseWT39.bam -o /cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/composite_bam2counts/ParseWT39\n",
    "utils/composite_bam2counts -b/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/translated_ParseWT40.bam -o /cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/composite_bam2counts/ParseWT40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c286b1a",
   "metadata": {},
   "source": [
    "### 5. Outputs processed with demux_species –o [output_directory]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5cd63",
   "metadata": {},
   "source": [
    "### 6. Created a metadata with parse barcodes, sudo10X barcodes, species assigned, singlet/doublet score, and confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb761162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ParseWT samples to process\n",
    "samples = ['ParseWT21', 'ParseWT22', 'ParseWT33', 'ParseWT34', 'ParseWT35', \n",
    "           'ParseWT36', 'ParseWT37', 'ParseWT38', 'ParseWT39', 'ParseWT40']\n",
    "\n",
    "for sample in samples:\n",
    "    print(f\"Processing {sample}...\")\n",
    "    \n",
    "    # Read barcode translation file (CSV)\n",
    "    barcode_df = pd.read_csv(f\"/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/translated_{sample}_barcodes.csv\", \n",
    "                            header=None,\n",
    "                            skiprows=1,\n",
    "                            names=['new_barcode','old_barcode'])\n",
    "    \n",
    "    # Read species assignments file (tab-delimited)\n",
    "    species_df = pd.read_csv(f\"/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/composite_bam2counts/{sample}/species.assignments\", \n",
    "                            sep='\\t',\n",
    "                            header=None,\n",
    "                            names=['new_barcode', 'species_predicted', 'doublet_singlet_score', 'confidence'])\n",
    "    \n",
    "    # Merge on new_barcode\n",
    "    merged_df = pd.merge(barcode_df, species_df, on='new_barcode', how='outer')\n",
    "    \n",
    "    # Save result\n",
    "    merged_df.to_csv(f\"metadata_barcode_species_{sample}.csv\", index=False)\n",
    "\n",
    "print(\"All samples processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a1c03",
   "metadata": {},
   "source": [
    "### 7. Created list of barcodes of each species for each sublibrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba7d88be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ParseWT21...\n",
      "Processing ParseWT22...\n",
      "Processing ParseWT33...\n",
      "Processing ParseWT34...\n",
      "Processing ParseWT35...\n",
      "Processing ParseWT36...\n",
      "Processing ParseWT37...\n",
      "Processing ParseWT38...\n",
      "Processing ParseWT39...\n",
      "Processing ParseWT40...\n"
     ]
    }
   ],
   "source": [
    "samples = ['ParseWT21', 'ParseWT22', 'ParseWT33', 'ParseWT34', 'ParseWT35', \n",
    "           'ParseWT36', 'ParseWT37', 'ParseWT38', 'ParseWT39', 'ParseWT40']\n",
    "\n",
    "meta = {}\n",
    "for sample in samples:\n",
    "    print(f\"Processing {sample}...\")\n",
    "    meta[sample] = pd.read_csv(f\"/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/demultiplexed/metadata_barcode_species_{sample}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82945bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_s = {}\n",
    "for sample in meta:\n",
    "    meta_s[sample] = meta[sample][meta[sample][\"doublet_singlet_score\"] == \"S\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f73b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParseWT21 dimensions: 878833 rows, 5 columns\n",
      "ParseWT22 dimensions: 875617 rows, 5 columns\n",
      "ParseWT33 dimensions: 874555 rows, 5 columns\n",
      "ParseWT34 dimensions: 882171 rows, 5 columns\n",
      "ParseWT35 dimensions: 882650 rows, 5 columns\n",
      "ParseWT36 dimensions: 882809 rows, 5 columns\n",
      "ParseWT37 dimensions: 883160 rows, 5 columns\n",
      "ParseWT38 dimensions: 882919 rows, 5 columns\n",
      "ParseWT39 dimensions: 883527 rows, 5 columns\n",
      "ParseWT40 dimensions: 881903 rows, 5 columns\n"
     ]
    }
   ],
   "source": [
    "for sample in meta:\n",
    "    print(f\"{sample} dimensions: {meta[sample].shape[0]} rows, {meta[sample].shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c6117dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_human = {}\n",
    "for sample in meta_s:\n",
    "   meta_human[sample] = meta_s[sample][meta_s[sample][\"species_predicted\"] == \"GRCh38\"]\n",
    "\n",
    "meta_gorila = {}\n",
    "for sample in meta_s:\n",
    "   meta_gorila[sample] = meta_s[sample][meta_s[sample][\"species_predicted\"] == \"gorGor4\"]\n",
    "\n",
    "meta_chimp = {}\n",
    "for sample in meta_s:\n",
    "   meta_chimp[sample] = meta_s[sample][meta_s[sample][\"species_predicted\"] == \"Pan-tro-3-0\"]\n",
    "\n",
    "meta_macaque = {}\n",
    "for sample in meta_s:\n",
    "   meta_macaque[sample] = meta_s[sample][meta_s[sample][\"species_predicted\"] == \"Macaca-fascicularis-6-0\"]\n",
    "\n",
    "meta_mouse = {}\n",
    "for sample in meta_s:\n",
    "   meta_mouse[sample] = meta_s[sample][meta_s[sample][\"species_predicted\"] == \"GRCm39\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3558dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HUMAN:\n",
      "  ParseWT21 dimensions: 154885 rows, 5 columns\n",
      "  ParseWT22 dimensions: 194135 rows, 5 columns\n",
      "  ParseWT33 dimensions: 302092 rows, 5 columns\n",
      "  ParseWT34 dimensions: 184830 rows, 5 columns\n",
      "  ParseWT35 dimensions: 320893 rows, 5 columns\n",
      "  ParseWT36 dimensions: 172378 rows, 5 columns\n",
      "  ParseWT37 dimensions: 3996 rows, 5 columns\n",
      "  ParseWT38 dimensions: 4090 rows, 5 columns\n",
      "  ParseWT39 dimensions: 454671 rows, 5 columns\n",
      "  ParseWT40 dimensions: 6853 rows, 5 columns\n",
      "\n",
      "GORILA:\n",
      "  ParseWT21 dimensions: 27113 rows, 5 columns\n",
      "  ParseWT22 dimensions: 4455 rows, 5 columns\n",
      "  ParseWT33 dimensions: 33719 rows, 5 columns\n",
      "  ParseWT34 dimensions: 30260 rows, 5 columns\n",
      "  ParseWT35 dimensions: 2147 rows, 5 columns\n",
      "  ParseWT36 dimensions: 31857 rows, 5 columns\n",
      "  ParseWT37 dimensions: 186270 rows, 5 columns\n",
      "  ParseWT38 dimensions: 238043 rows, 5 columns\n",
      "  ParseWT39 dimensions: 1520 rows, 5 columns\n",
      "  ParseWT40 dimensions: 3126 rows, 5 columns\n",
      "\n",
      "CHIMP:\n",
      "  ParseWT21 dimensions: 37548 rows, 5 columns\n",
      "  ParseWT22 dimensions: 43861 rows, 5 columns\n",
      "  ParseWT33 dimensions: 38802 rows, 5 columns\n",
      "  ParseWT34 dimensions: 35174 rows, 5 columns\n",
      "  ParseWT35 dimensions: 39393 rows, 5 columns\n",
      "  ParseWT36 dimensions: 31086 rows, 5 columns\n",
      "  ParseWT37 dimensions: 1970 rows, 5 columns\n",
      "  ParseWT38 dimensions: 2146 rows, 5 columns\n",
      "  ParseWT39 dimensions: 1350 rows, 5 columns\n",
      "  ParseWT40 dimensions: 3789 rows, 5 columns\n",
      "\n",
      "MACAQUE:\n",
      "  ParseWT21 dimensions: 20704 rows, 5 columns\n",
      "  ParseWT22 dimensions: 34537 rows, 5 columns\n",
      "  ParseWT33 dimensions: 25567 rows, 5 columns\n",
      "  ParseWT34 dimensions: 21755 rows, 5 columns\n",
      "  ParseWT35 dimensions: 21472 rows, 5 columns\n",
      "  ParseWT36 dimensions: 17594 rows, 5 columns\n",
      "  ParseWT37 dimensions: 22164 rows, 5 columns\n",
      "  ParseWT38 dimensions: 19650 rows, 5 columns\n",
      "  ParseWT39 dimensions: 20046 rows, 5 columns\n",
      "  ParseWT40 dimensions: 32120 rows, 5 columns\n",
      "\n",
      "MOUSE:\n",
      "  ParseWT21 dimensions: 22529 rows, 5 columns\n",
      "  ParseWT22 dimensions: 27248 rows, 5 columns\n",
      "  ParseWT33 dimensions: 23195 rows, 5 columns\n",
      "  ParseWT34 dimensions: 22322 rows, 5 columns\n",
      "  ParseWT35 dimensions: 21304 rows, 5 columns\n",
      "  ParseWT36 dimensions: 21242 rows, 5 columns\n",
      "  ParseWT37 dimensions: 24561 rows, 5 columns\n",
      "  ParseWT38 dimensions: 21507 rows, 5 columns\n",
      "  ParseWT39 dimensions: 20555 rows, 5 columns\n",
      "  ParseWT40 dimensions: 23298 rows, 5 columns\n"
     ]
    }
   ],
   "source": [
    "meta_species = {'human': meta_human, 'gorila': meta_gorila, 'chimp': meta_chimp, 'macaque': meta_macaque, 'mouse': meta_mouse}\n",
    "for species_name, meta_dict in meta_species.items():\n",
    "    print(f\"\\n{species_name.upper()}:\")\n",
    "    for sample in meta_dict:\n",
    "        print(f\"  {sample} dimensions: {meta_dict[sample].shape[0]} rows, {meta_dict[sample].shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ef6f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_data = {\n",
    "   'human': meta_human,\n",
    "   'gorilla': meta_gorila,\n",
    "   'chimp': meta_chimp,\n",
    "   'macaque': meta_macaque,\n",
    "   'mouse': meta_mouse\n",
    "}\n",
    "\n",
    "# Loop through each species\n",
    "for species_name, species_meta in species_data.items():\n",
    "   for sample in species_meta:\n",
    "       barcodes = species_meta[sample]['new_barcode']\n",
    "       barcodes.to_csv(f\"{sample}_{species_name}_barcodes.csv\", index=False, header=['new_barcode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf83b8e0",
   "metadata": {},
   "source": [
    "### 8. Filtering FASTQ files into separate species. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23af51",
   "metadata": {},
   "source": [
    "#### 8.1 Making lists of headers corresponding to each species per sublibarary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165558fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nohup python -c \"\n",
    "import pysam\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "barcode_dir = '/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/demultiplexed'\n",
    "bam_dir = '/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/modify_bam/modified_bam'\n",
    "output_dir = '/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/demultiplexed/Fastq_filters'\n",
    "\n",
    "# Define sublibraries and species\n",
    "sublibraries = ['ParseWT21', 'ParseWT22', 'ParseWT33', 'ParseWT34', 'ParseWT35', \n",
    "               'ParseWT36', 'ParseWT37', 'ParseWT38', 'ParseWT39', 'ParseWT40']\n",
    "species = ['human', 'chimp', 'gorilla', 'macaque', 'mouse']\n",
    "\n",
    "for sublibrary in sublibraries:\n",
    "    print(f'Processing {sublibrary}...', flush=True)\n",
    "    \n",
    "    bam_file = f'{bam_dir}/translated_{sublibrary}.bam'\n",
    "    \n",
    "    for sp in species:\n",
    "        print(f'  Processing {sp}...', flush=True)\n",
    "        \n",
    "        barcode_file = f'{barcode_dir}/{sublibrary}_{sp}_barcodes.csv'\n",
    "        \n",
    "        barcodes_to_find = set()\n",
    "        with open(barcode_file, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                barcodes_to_find.add(row[0])\n",
    "        \n",
    "        matching_headers = []\n",
    "        with pysam.AlignmentFile(bam_file, 'rb') as bam:\n",
    "            for read in bam:\n",
    "                if read.has_tag('CB') and read.get_tag('CB') in barcodes_to_find:\n",
    "                    header = read.query_name\n",
    "                    if '@' in header:\n",
    "                        header = header.split('@')[1].split(',')[0]\n",
    "                    matching_headers.append(header)\n",
    "        \n",
    "        output_file = f'{output_dir}/headers_{sp}_{sublibrary}.csv'\n",
    "        with open(output_file, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['header'])\n",
    "            for header in matching_headers:\n",
    "                writer.writerow([header])\n",
    "        \n",
    "        print(f'    Found {len(matching_headers)} headers -> {output_file}', flush=True)\n",
    "\n",
    "print('All processing complete!', flush=True)\n",
    "\" > bam_processing.log 2>&1 &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3167b5",
   "metadata": {},
   "source": [
    "#### 8.2 Concatanting FASTQ of each sublibrary across sequencing lanes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /cephfs2/hannas/MultiSpeciesComp/final/expdata/Fastq_merged\n",
    " \n",
    "for i in {21..22}; do echo $i; cat SLX-25498.ParseWT$i.*.r_1.fq.gz > SLX-25498.ParseWT$i.22K75GLT4.combined.r_1.fq.gz& done\n",
    "\n",
    "for i in {21..22}; do echo $i; cat SLX-25498.ParseWT$i.*.r_2.fq.gz > SLX-25498.ParseWT$i.22K75GLT4.combined.r_2.fq.gz& done\n",
    "\n",
    "for i in {33..40}; do echo $i; cat SLX-25498.ParseWT$i.*.r_1.fq.gz > SLX-25498.ParseWT$i.22K75GLT4.combined.r_1.fq.gz& done\n",
    "\n",
    "for i in {33..40}; do echo $i; cat SLX-25498.ParseWT$i.*.r_2.fq.gz > SLX-25498.ParseWT$i.22K75GLT4.combined.r_2.fq.gz& done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fb7e3e",
   "metadata": {},
   "source": [
    "#### 8.3 Filtering the fastq with header lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3910c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved as filter_merged_fastq.py at /cephfs2/hannas/MultiSpeciesComp/final/expdata/Fastq_filtered/filter_merged_fastq.py\n",
    "import os\n",
    "import csv\n",
    "import gzip\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def load_headers_from_csv(csv_file):\n",
    "    \"\"\"Load headers from CSV into a set for fast lookup\"\"\"\n",
    "    headers = set()\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            headers.add(row[0])\n",
    "    return headers\n",
    "\n",
    "def filter_fastq_gz(input_fastq, output_fastq, headers_to_keep):\n",
    "    \"\"\"Filter compressed FASTQ file keeping only reads with headers in the set\"\"\"\n",
    "    kept_reads = 0\n",
    "    \n",
    "    with gzip.open(input_fastq, 'rt') as infile, gzip.open(output_fastq, 'wt') as outfile:\n",
    "        while True:\n",
    "            # Read FASTQ record (4 lines)\n",
    "            header = infile.readline().strip()\n",
    "            if not header:  # End of file\n",
    "                break\n",
    "            sequence = infile.readline().strip()\n",
    "            plus = infile.readline().strip()\n",
    "            quality = infile.readline().strip()\n",
    "            \n",
    "            # Extract read ID from header (remove @ and everything after first space)\n",
    "            read_id = header[1:].split()[0]\n",
    "            \n",
    "            # Keep read if header matches (saves as FASTQ format with quality scores)\n",
    "            if read_id in headers_to_keep:\n",
    "                outfile.write(f\"{header}\\n{sequence}\\n{plus}\\n{quality}\\n\")\n",
    "                kept_reads += 1\n",
    "    \n",
    "    return kept_reads\n",
    "\n",
    "def process_single_task(task):\n",
    "    \"\"\"Process a single FASTQ filtering task\"\"\"\n",
    "    fastq_file, header_file, output_file, sublibrary, species, read_type = task\n",
    "    \n",
    "    try:\n",
    "        # Load headers INSIDE the worker process to avoid memory duplication\n",
    "        headers_to_keep = load_headers_from_csv(header_file)\n",
    "        \n",
    "        # Filter FASTQ\n",
    "        kept_reads = filter_fastq_gz(fastq_file, output_file, headers_to_keep)\n",
    "        \n",
    "        # Clear headers from memory immediately\n",
    "        del headers_to_keep\n",
    "        \n",
    "        return {\n",
    "            'sublibrary': sublibrary,\n",
    "            'species': species,\n",
    "            'read_type': read_type,\n",
    "            'output_file': os.path.basename(output_file),\n",
    "            'kept_reads': kept_reads,\n",
    "            'status': 'success'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'sublibrary': sublibrary,\n",
    "            'species': species,\n",
    "            'read_type': read_type,\n",
    "            'error': str(e),\n",
    "            'status': 'error'\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    print(\"Starting FASTQ filtering job...\")\n",
    "    \n",
    "    # Configuration\n",
    "    merged_fastq_dir = \"/cephfs2/hannas/MultiSpeciesComp/final/expdata/Fastq_merged\"\n",
    "    header_dir = \"/cephfs2/hannas/MultiSpeciesComp/final/analysis/demultiplexing_cellbouncer/demultiplexed/Fastq_filters\"\n",
    "    output_dir = \"/cephfs2/hannas/MultiSpeciesComp/final/expdata/Fastq_filtered\"\n",
    "    \n",
    "    # Use fewer workers to reduce memory pressure\n",
    "    max_workers = min(int(os.environ.get('SLURM_CPUS_PER_TASK', cpu_count())), 8)\n",
    "    \n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"Using {max_workers} parallel workers (reduced for memory)\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    sublibraries = [\"ParseWT21\", \"ParseWT22\", \"ParseWT33\", \"ParseWT34\", \"ParseWT35\", \n",
    "                   \"ParseWT36\", \"ParseWT37\", \"ParseWT38\", \"ParseWT39\", \"ParseWT40\"]\n",
    "    species = [\"human\", \"chimp\", \"gorilla\", \"macaque\", \"mouse\"]\n",
    "    \n",
    "    # Build task list\n",
    "    tasks = []\n",
    "    for sublibrary in sublibraries:\n",
    "        for sp in species:\n",
    "            # Check if header file exists\n",
    "            header_file = f\"{header_dir}/headers_{sp}_{sublibrary}.csv\"\n",
    "            if not os.path.exists(header_file):\n",
    "                print(f\"Skipping {sublibrary} {sp}: header file not found - {header_file}\")\n",
    "                continue\n",
    "            \n",
    "            # Process R1 file - updated filename format\n",
    "            r1_file = f\"{merged_fastq_dir}/SLX-25498.{sublibrary}.22K75GLT4.combined.r_1.fq.gz\"\n",
    "            if os.path.exists(r1_file):\n",
    "                output_r1 = f\"{output_dir}/{sublibrary}_{sp}_R1.fq.gz\"\n",
    "                tasks.append((r1_file, header_file, output_r1, sublibrary, sp, \"R1\"))\n",
    "            else:\n",
    "                print(f\"R1 file not found: {r1_file}\")\n",
    "            \n",
    "            # Process R2 file - updated filename format\n",
    "            r2_file = f\"{merged_fastq_dir}/SLX-25498.{sublibrary}.22K75GLT4.combined.r_2.fq.gz\"\n",
    "            if os.path.exists(r2_file):\n",
    "                output_r2 = f\"{output_dir}/{sublibrary}_{sp}_R2.fq.gz\"\n",
    "                tasks.append((r2_file, header_file, output_r2, sublibrary, sp, \"R2\"))\n",
    "            else:\n",
    "                print(f\"R2 file not found: {r2_file}\")\n",
    "    \n",
    "    print(f\"Found {len(tasks)} filtering tasks to process\")\n",
    "    \n",
    "    if not tasks:\n",
    "        print(\"No tasks found. Check file paths and header files.\")\n",
    "        return\n",
    "    \n",
    "    # Process tasks in parallel with reduced concurrency\n",
    "    completed = 0\n",
    "    total_reads = 0\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_task = {executor.submit(process_single_task, task): task for task in tasks}\n",
    "        \n",
    "        for future in as_completed(future_to_task):\n",
    "            result = future.result()\n",
    "            completed += 1\n",
    "            \n",
    "            if result['status'] == 'success':\n",
    "                total_reads += result['kept_reads']\n",
    "                print(f\"[{completed}/{len(tasks)}] {result['sublibrary']} {result['species']} \"\n",
    "                      f\"{result['read_type']}: {result['kept_reads']} reads -> {result['output_file']}\")\n",
    "            else:\n",
    "                print(f\"[{completed}/{len(tasks)}] ERROR {result['sublibrary']} {result['species']} \"\n",
    "                      f\"{result['read_type']}: {result['error']}\")\n",
    "    \n",
    "    print(f\"\\nAll filtering complete!\")\n",
    "    print(f\"Total reads kept: {total_reads}\")\n",
    "    print(f\"Files saved to: {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e8ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=filter_fastq\n",
    "#SBATCH --mem=128G\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --time=12:00:00\n",
    "#SBATCH --output=filter_fastq_%j.out\n",
    "#SBATCH --error=filter_fastq_%j.err\n",
    "\n",
    "# Load Python if needed (adjust for your cluster)\n",
    "# module load python/3.9  # Uncomment if needed\n",
    "\n",
    "echo \"Starting FASTQ filtering job at: $(date)\"\n",
    "echo \"Job ID: $SLURM_JOB_ID\"\n",
    "echo \"CPUs allocated: $SLURM_CPUS_PER_TASK\"\n",
    "echo \"Memory allocated: $SLURM_MEM_PER_NODE MB\"\n",
    "\n",
    "# Change to the script directory\n",
    "cd /cephfs2/hannas/MultiSpeciesComp/final/expdata/Fastq_filtered\n",
    "\n",
    "# Run the Python script\n",
    "python3 filter_merged_fastq.py\n",
    "\n",
    "echo \"Job completed at: $(date)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc025ffe",
   "metadata": {},
   "source": [
    "### 9. Realignment of species FASTQ to species-specific reference genomes using ParseNIP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runing split-pipe thoutgh ParseNIP\n",
    "nextflow run -config /public/singularity/containers/nextflow/lmb-nextflow/genomes.config,/public/singularity/containers/nextflow/ParseNIP/nextflow.config \\\n",
    "-profile lmb_cluster /public/singularity/containers/nextflow/ParseNIP/main.nf \\\n",
    "-genome homo_sapiens.GRCh38.release_102 --genome_name homo_sapiens.GRCh38.release_102 \\\n",
    "--samp_list /cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/SampleLoadingTable_MultiSpieciesComp.txt \\\n",
    "--fastq /cephfs2/hannas/MultiSpeciesComp/final/expdata/Fastq_filtered/human --chemistry v1 -bg\n",
    "\n",
    "nextflow run -config /public/singularity/containers/nextflow/lmb-nextflow/genomes.config,/public/singularity/containers/nextflow/ParseNIP/nextflow.config \\\n",
    "-profile lmb_cluster /public/singularity/containers/nextflow/ParseNIP/main.nf \\\n",
    "-genome pan_troglodytes.Pan_tro_3.0.release_105 --genome_name pan_troglodytes.Pan_tro_3.0.release_105 \\\n",
    "--samp_list /cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/SampleLoadingTable_MultiSpieciesComp.txt \\\n",
    "--fastq /cephfs2/hannas/MultiSpeciesComp/final/expdata/Fastq_filtered/chimp --chemistry v1 -bg\n",
    "\n",
    "nextflow run -config /public/singularity/containers/nextflow/lmb-nextflow/genomes.config,/public/singularity/containers/nextflow/ParseNIP/nextflow.config \\\n",
    "-profile lmb_cluster /public/singularity/containers/nextflow/ParseNIP/main.nf \\\n",
    "-genome gorilla_gorilla.gorGor4.release_110 --genome_name gorilla_gorilla.gorGor4.release_110 \\\n",
    "    --samp_list /cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/SampleLoadingTable_MultiSpieciesComp.txt \\\n",
    "--fastq /cephfs2/hannas/MultiSpeciesComp/final/expdata/Fastq_filtered/gorilla --chemistry v1 -bg\n",
    "\n",
    "nextflow run -config /public/singularity/containers/nextflow/lmb-nextflow/genomes.config,/public/singularity/containers/nextflow/ParseNIP/nextflow.config \\\n",
    "-profile lmb_cluster /public/singularity/containers/nextflow/ParseNIP/main.nf \\\n",
    "-genome macaca_fascicularis.Macaca_fascicularis_6.0.release_112 --genome_name macaca_fascicularis.Macaca_fascicularis_6.0.release_112 \\\n",
    "--samp_list /cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/SampleLoadingTable_MultiSpieciesComp.txt \\\n",
    "--fastq /cephfs2/hannas/MultiSpeciesComp/final/expdata/Fastq_filtered/macaque --chemistry v1 -bg\n",
    "\n",
    "nextflow run -config /public/singularity/containers/nextflow/lmb-nextflow/genomes.config,/public/singularity/containers/nextflow/ParseNIP/nextflow.config \\\n",
    "-profile lmb_cluster /public/singularity/containers/nextflow/ParseNIP/main.nf \\\n",
    "-genome mus_musculus.GRCm39.release_108 --genome_name mus_musculus.GRCm39.release_108 \\\n",
    "--samp_list /cephfs2/hannas/MultiSpeciesComp/final/expdata/Parsnip_mixedref_v2/SampleLoadingTable_MultiSpieciesComp.txt \\\n",
    "--fastq /cephfs2/hannas/MultiSpeciesComp/final/expdata/Fastq_filtered/mouse --chemistry v1 -bgbg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfcfa0f",
   "metadata": {},
   "source": [
    "### 10. CrossFilt removal of annotation biases from resulting BAM files: crossfilt-lift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e88d3",
   "metadata": {},
   "source": [
    "CrossFilt remove the sequences do nto reciprcally match between the species. \n",
    "It alignes reads from target bam to the target genome (the species we are converting from). \n",
    "Next, each nucleotide in the read that matches the target genome is replaced with the corresponding nucleotide from the query genome, accounting for insertions and deletions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2fa7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /cephfs2/hannas/MultiSpeciesComp/final/genome/CrossFilt\n",
    "conda activate crossfilt\n",
    "\n",
    "#getign chain files from USCS \n",
    "wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/liftOver/hg38ToPanTro6.over.chain.gz\n",
    "wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/liftOver/hg38ToGorGor5.over.chain.gz\n",
    "wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/liftOver/hg38ToMacFas5.over.chain.gz\n",
    "wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/liftOver/hg38ToMm39.over.chain.gz\n",
    "\n",
    "#crossfilt-lift commands\n",
    "CHAIN_dir = '/cephfs2/hannas/MultiSpeciesComp/final/genome/CrossFilt/chain_files'\n",
    "ENSEMBLE_dir = '/cephfs2/hannas/MultiSpeciesComp/final/genome/Esemble_genomes'\n",
    "\n",
    "crossfilt-lift -i INPUT -o 'crossfilt_chimp2human' -c $CHAIN_dir/hg38ToPanTro6.over.chain.gz -t $ENSEMBLE_dir/pan_troglodytes/Pan_tro_3.0/Release_105/FASTA/pan_troglodytes__Pan_tro_3.0__release105.dna.fa -q $ENSEMBLE_dir/homo_sapiens/GRCh38/Release_102/FASTA/homo_sapiens__GRCh38__release102.dna.fa -p\n",
    "crossfilt-lift -i INPUT -o 'crossfilt_gorila2human' -c $CHAIN_dir/hg38ToGorGor5.over.chain.gz -t $ENSEMBLE_dir/gorilla_gorilla/gorGor4/Release_110/FASTA/gorilla_gorilla__gorGor4__release110.dna.fa -q $ENSEMBLE_dir/homo_sapiens/GRCh38/Release_102/FASTA/homo_sapiens__GRCh38__release102.dna.fa -p\n",
    "crossfilt-lift -i INPUT -o 'crossfilt_macaque2human' -c $CHAIN_dir/hg38ToMacFas5.over.chain.gz -t $ENSEMBLE_dir/macaca_mulatta/Mmul_10/Release_105/FASTA/macaca_mulatta__Mmul_10__release105.dna.fa -q $ENSEMBLE_dir/homo_sapiens/GRCh38/Release_102/FASTA/homo_sapiens__GRCh38__release102.dna.fa -p\n",
    "crossfilt-lift -i INPUT -o 'crossfilt_mouse2human' -c $CHAIN_dir/hg38ToMm39.over.chain.gz -t $ENSEMBLE_dir/mus_musculus/GRCm39/Release_108/FASTA/mus_musculus__GRCm39__release108.dna.fa -q $ENSEMBLE_dir/homo_sapiens/GRCh38/Release_102/FASTA/homo_sapiens__GRCh38__release102.dna.fa -p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
